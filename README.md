# Hive_Spark_Docker_Project

In this project, I constructed a reliable data pipeline using Apache Spark, Docker, and Hive. I start an AWS EC2 instance and install Hive and a Docker container on it. Then, using Pyspark, connect Apache Spark to Hive, where I am running an ETL pipeline.

# Note

- I use Apache and Hive for faster data processing
- And Hive is modern alternative to MapReduce.

# Dataset tools used

- AWS EC2 Instance
- Hadoop Hive
- Apache Spark
- Docker
- Jupyter Notebook

# Programming Language

- Python
